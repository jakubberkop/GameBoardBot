diff --git a/.vscode/launch.json b/.vscode/launch.json
index 4864f3d..2661aaf 100644
--- a/.vscode/launch.json
+++ b/.vscode/launch.json
@@ -21,10 +21,10 @@
 			"justMyCode": true
 		},
 		{
-			"name": "Python: train.py",
+			"name": "Python: AA.py",
 			"type": "python",
 			"request": "launch",
-			"program": "train.py",
+			"program": "AA.py",
 			"console": "integratedTerminal",
 			"justMyCode": true
 		}
diff --git a/AA.py b/AA.py
index 841b66f..cd288cd 100644
--- a/AA.py
+++ b/AA.py
@@ -1,10 +1,11 @@
+from typing import List
 import gymnasium as gym
 import math
 import random
-import matplotlib
 import matplotlib.pyplot as plt
 from collections import namedtuple, deque
 from itertools import count
+import numpy as np
 
 import torch
 import torch.nn as nn
@@ -13,70 +14,36 @@ import torch.nn.functional as F
 from game import get_game_score
 
 import game_env
-
-
-import pathlib
-import pathlib
 import datetime
-import glob
-
-# import pytracy
-# pytracy.set_tracing_mode(pytracy.TracingMode.All)
+import pathlib
+import tqdm
 
-# env = gym.make("CartPole-v1")
+from torch.utils.tensorboard import SummaryWriter
 
 gym.register("GAME", entry_point=game_env.GameEnv, max_episode_steps=10000)
-env = gym.make("GAME")
-
-# set up matplotlib
-is_ipython = 'inline' in matplotlib.get_backend()
-if is_ipython:
-    from IPython import display
-
 plt.ion()
 
 # if GPU is to be used
-device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
-# device = "cpu"
-
-num_episodes = 10000
-show_graph_frequency = 50
-
-######################################################################
-# Replay Memory
-# -------------
-#
-# We'll be using experience replay memory for training our DQN. It stores
-# the transitions that the agent observes, allowing us to reuse this data
-# later. By sampling from it randomly, the transitions that build up a
-# batch are decorrelated. It has been shown that this greatly stabilizes
-# and improves the DQN training procedure.
-#
-# For this, we're going to need two classes:
-#
-# -  ``Transition`` - a named tuple representing a single transition in
-#    our environment. It essentially maps (state, action) pairs
-#    to their (next_state, reward) result, with the state being the
-#    screen difference image as described later on.
-# -  ``ReplayMemory`` - a cyclic buffer of bounded size that holds the
-#    transitions observed recently. It also implements a ``.sample()``
-#    method for selecting a random batch of transitions for training.
-#
+DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
+# DEVICE = "cpu"
+print(f"Using device: {DEVICE}")
 
 Transition = namedtuple('Transition',
                         ('state', 'action', 'next_state', 'reward'))
 
 
+tensorboard_writer = SummaryWriter()
+
 class ReplayMemory(object):
 
-    def __init__(self, capacity):
-        self.memory = deque([], maxlen=capacity)
+    def __init__(self, capacity: int):
+        self.memory: deque[object] = deque([], maxlen=capacity)
 
     def push(self, *args):
         """Save a transition"""
         self.memory.append(Transition(*args))
 
-    def sample(self, batch_size):
+    def sample(self, batch_size: int):
         return random.sample(self.memory, batch_size)
 
     def __len__(self):
@@ -89,7 +56,7 @@ class DQN(nn.Module):
         super(DQN, self).__init__()
         self.layer1 = nn.Linear(n_observations, 128)
         self.layer2 = nn.Linear(128, 128)
-        self.layer3 = nn.Linear(128, 128)
+        # self.layer3 = nn.Linear(128, 128)
         self.layer4 = nn.Linear(128, 128)
         self.layer5 = nn.Linear(128, n_actions)
 
@@ -98,42 +65,16 @@ class DQN(nn.Module):
     def forward(self, x):
         x = F.relu(self.layer1(x))
         x = F.relu(self.layer2(x))
-        x = F.relu(self.layer3(x))
+        # x = F.relu(self.layer3(x))
         x = F.relu(self.layer4(x))
         x = F.softmax(self.layer5(x), dim=1)
 
         return x
 
+NUM_EPISODES = 60000
+SHOW_GRAPH_FREQUENCY = 30
 
-######################################################################
-# Training
-# --------
-#
-# Hyperparameters and utilities
-# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-# This cell instantiates our model and its optimizer, and defines some
-# utilities:
-#
-# -  ``select_action`` - will select an action accordingly to an epsilon
-#    greedy policy. Simply put, we'll sometimes use our model for choosing
-#    the action, and sometimes we'll just sample one uniformly. The
-#    probability of choosing a random action will start at ``EPS_START``
-#    and will decay exponentially towards ``EPS_END``. ``EPS_DECAY``
-#    controls the rate of the decay.
-# -  ``plot_scores`` - a helper for plotting the score of episodes,
-#    along with an average over the last 100 episodes (the measure used in
-#    the official evaluations). The plot will be underneath the cell
-#    containing the main training loop, and will update after every
-#    episode.
-#
-
-# BATCH_SIZE is the number of transitions sampled from the replay buffer
-# GAMMA is the discount factor as mentioned in the previous section
-# EPS_START is the starting value of epsilon
-# EPS_END is the final value of epsilon
-# EPS_DECAY controls the rate of exponential decay of epsilon, higher means a slower decay
-# TAU is the update rate of the target network
-# LR is the learning rate of the ``AdamW`` optimizer
+# Learning
 BATCH_SIZE = 128
 GAMMA = 0.99
 EPS_START = 0.9
@@ -141,253 +82,273 @@ EPS_END = 0.05
 EPS_DECAY = 1000
 TAU = 0.005
 LR = 1e-4
+SHOULD_SAVE_RESULTS = True
+VERBOSE = False
+TRACY = False
 
-# Get number of actions from gym action space
-n_actions = env.action_space.n
-# Get the number of state observations
-state, info = env.reset()
-n_observations = len(state)
 
-policy_net = DQN(n_observations, n_actions).to(device)
+if TRACY:
+    import pytracy
+    pytracy.set_tracing_mode(pytracy.TracingMode.All)
 
-# Load newest policy_net weights
 
+class Trainer:
 
-# Get the newest policy_net weights
-# policy_net_weights = sorted(glob.glob("results/*/policy_net.pt"))[-1]
-# print(f"Loading policy_net weights from {policy_net_weights}")
-# policy_net.load_state_dict(torch.load(policy_net_weights))
+    def __init__(self):
+        self.steps_done: int = 0
+        self.env: game_env.GameEnv = gym.make("GAME", render_mode="human" if VERBOSE else None)
 
-target_net = DQN(n_observations, n_actions).to(device)
-target_net.load_state_dict(policy_net.state_dict())
+        # Get number of actions from gym action space
+        n_actions: int = int(self.env.action_space.n)
+        # Get the number of state observations
+        state, info = self.env.reset()
+        n_observations = len(state)
 
-optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)
-memory = ReplayMemory(10000)
+        self.scores: List[float] = []
+        self.game_lengths: List[int] = []
+        self.actions: List[int] = []
+        self.rewards: List[int] = []
 
+        self.policy_net = DQN(n_observations, n_actions).to(DEVICE)
+        self.target_net = DQN(n_observations, n_actions).to(DEVICE)
 
-steps_done = 0
+        self.target_net.load_state_dict(self.policy_net.state_dict())
 
+        self.optimizer = optim.AdamW(self.policy_net.parameters(), lr=LR, amsgrad=True)
+        self.memory = ReplayMemory(10000)
 
-def select_action(state):
-    global steps_done
-    sample = random.random()
-    eps_threshold = EPS_END + (EPS_START - EPS_END) * \
-        math.exp(-1. * steps_done / EPS_DECAY)
-    steps_done += 1
-    if sample > eps_threshold:
-        with torch.no_grad():
-            # t.max(1) will return the largest column value of each row.
-            # second column on max result is index of where max element was
-            # found, so we pick action with the larger expected reward.
-            return policy_net(state).max(1).indices.view(1, 1)
-    else:
-        return torch.tensor([[env.action_space.sample()]], device=device, dtype=torch.float32)
+    def select_action(self, state: torch.Tensor, legal_moves: np.ndarray) -> torch.Tensor: #TODO: MAKE IT an int (faster)
+        sample = random.random()
+        eps_threshold = EPS_END + (EPS_START - EPS_END) * \
+            math.exp(-1. * self.steps_done / EPS_DECAY)
+        self.steps_done += 1
 
+        assert legal_moves.sum() > 0, "No legal moves"
 
-scores = []
-game_lengths = []
+        if sample > eps_threshold:
+            with torch.no_grad():
+                # t.max(1) will return the largest column value of each row.
+                # second column on max result is index of where max element was
+                # found, so we pick action with the larger expected reward.
+                policy_net_output = self.policy_net(state)
 
+                # Apply the mask to the input tensor
+                masked_input = policy_net_output.masked_fill(~torch.tensor(legal_moves, dtype=torch.bool, device=DEVICE), float('-inf'))
 
-def plot_scores(show_result=False):
-    plt.figure(1)
-    scores_t = torch.tensor(scores, dtype=torch.float)
-    game_lengths_t = torch.tensor(game_lengths, dtype=torch.float)
-    if show_result:
-        plt.title('Result')
-    else:
-        plt.clf()
-        plt.title('Training...')
-    plt.xlabel('Episode')
-    plt.ylabel('STUFF')
-    plt.plot(scores_t.numpy(), label="scores")
-    plt.plot(game_lengths_t.numpy(), label="Game Lengths")
-    
-    
-    # Take 100 episode averages and plot them too
-    if len(scores_t) >= 100:
-        means = scores_t.unfold(0, 100, 1).mean(1).view(-1)
-        means = torch.cat((torch.zeros(99), means))
-        plt.plot(means.numpy(), label="MEAN scores", )
-
-        # Means of game lengths
-        mean_game_lengths = game_lengths_t.unfold(0, 100, 1).mean(1).view(-1)
-        mean_game_lengths = torch.cat((torch.zeros(99), mean_game_lengths))
-        plt.plot(mean_game_lengths.numpy(), label="MEAN Game Lengths", )
-
-    plt.legend()
-    plt.pause(0.01)  # pause a bit so that plots are updated
-    if is_ipython:
-        if not show_result:
-            display.display(plt.gcf())
-            display.clear_output(wait=True)
-        else:
-            display.display(plt.gcf())
-
-
-######################################################################
-# Training loop
-# ^^^^^^^^^^^^^
-#
-# Finally, the code for training our model.
-#
-# Here, you can find an ``optimize_model`` function that performs a
-# single step of the optimization. It first samples a batch, concatenates
-# all the tensors into a single one, computes :math:`Q(s_t, a_t)` and
-# :math:`V(s_{t+1}) = \max_a Q(s_{t+1}, a)`, and combines them into our
-# loss. By definition we set :math:`V(s) = 0` if :math:`s` is a terminal
-# state. We also use a target network to compute :math:`V(s_{t+1})` for
-# added stability. The target network is updated at every step with a 
-# `soft update <https://arxiv.org/pdf/1509.02971.pdf>`__ controlled by 
-# the hyperparameter ``TAU``, which was previously defined.
-#
-
-def optimize_model():
-    if len(memory) < BATCH_SIZE:
-        return
-    transitions = memory.sample(BATCH_SIZE)
-    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for
-    # detailed explanation). This converts batch-array of Transitions
-    # to Transition of batch-arrays.
-    batch = Transition(*zip(*transitions))
-
-    # Compute a mask of non-final states and concatenate the batch elements
-    # (a final state would've been the one after which simulation ended)
-    
-    not_none_next_states = [t for t in batch.next_state if t is not None]
-    not_none_next_count = len(not_none_next_states)
-    # print(f"not_none_next_count: {not_none_next_count}")
-
-    if not_none_next_count == 0:
-        # print("Skipping optimize_model() because not_none_next_state_count == 0")
-        return
-
-    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), device=device, dtype=torch.bool)
-
-    non_final_next_states = torch.cat(not_none_next_states)
-    state_batch = torch.cat(batch.state)
-    action_batch = torch.cat(batch.action)
-    reward_batch = torch.cat(batch.reward)
-
-    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the
-    # columns of actions taken. These are the actions which would've been taken
-    # for each batch state according to policy_net
-    # action_batch_int = action_batch.type(torch.int64)
-    state_action_values = policy_net(state_batch).gather(1, action_batch.type(torch.int64))
-
-    # Compute V(s_{t+1}) for all next states.
-    # Expected values of actions for non_final_next_states are computed based
-    # on the "older" target_net; selecting their best reward with max(1).values
-    # This is merged based on the mask, such that we'll have either the expected
-    # state value or 0 in case the state was final.
-    next_state_values = torch.zeros(BATCH_SIZE, device=device)
-    with torch.no_grad():
-        next_state_values[non_final_mask] = target_net(non_final_next_states).max(1).values
-    # Compute the expected Q values
-    expected_state_action_values = (next_state_values * GAMMA) + reward_batch
-
-    criterion = nn.L1Loss()
-    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))
-
-    # Optimize the model
-    optimizer.zero_grad()
-    loss.backward()
-    # In-place gradient clipping
-    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)
-    optimizer.step()
-
-
-######################################################################
-#
-# Below, you can find the main training loop. At the beginning we reset
-# the environment and obtain the initial ``state`` Tensor. Then, we sample
-# an action, execute it, observe the next state and the reward (always
-# 1), and optimize our model once. When the episode ends (our model
-# fails), we restart the loop.
-#
-# Below, `num_episodes` is set to 600 if a GPU is available, otherwise 50 
-# episodes are scheduled so training does not take too long. However, 50 
-# episodes is insufficient for to observe good performance on CartPole.
-# You should see the model constantly achieve 500 steps within 600 training 
-# episodes. Training RL agents can be a noisy process, so restarting training
-# can produce better results if convergence is not observed.
-#
+                # Perform argmax on the masked input
+                action = torch.argmax(masked_input)
 
+                # Print the result
 
+                # torch masked select
+                # https://pytorch.org/docs/stable/generated/torch.masked_select.html
 
-import tqdm
+                # legal_moves_tensor = torch.tensor(legal_moves, dtype=torch.float32, device=DEVICE)
+
+                # Mask the policy_net_output with the legal_moves
+                # masked_policy_net_output = policy_net_output * legal_moves_tensor
 
-for i_episode in tqdm.tqdm(range(num_episodes)):
-    # Initialize the environment and get it's state
-    state, info = env.reset()
-    state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)
-    for t in count():
-        action = select_action(state)
-        observation, reward, terminated, truncated, _ = env.step(action.item())
-        reward = torch.tensor([reward], device=device)
-        done = terminated or truncated
-
-        if terminated:
-            next_state = None
+                # Get the index of the highest value
+                # action = masked_policy_net_output.max(1).indices.view(1, 1).item()
+
+        else:
+            # Give legal_moves vector
+            action = self.env.action_space.sample(legal_moves)
+
+        assert legal_moves[action] == 1, "Illegal move"
+
+        return torch.tensor([[action]], device=DEVICE, dtype=torch.float32)
+
+    def load(self):
+        pass
+        # Load newest policy_net weights
+
+        # Get the newest policy_net weights
+        # policy_net_weights = sorted(glob.glob("results/*/policy_net.pt"))[-1]
+        # print(f"Loading policy_net weights from {policy_net_weights}")
+        # policy_net.load_state_dict(torch.load(policy_net_weights))
+
+
+    def optimize_model(self):
+        if len(self.memory) < BATCH_SIZE:
+            return
+        transitions = self.memory.sample(BATCH_SIZE)
+        # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for
+        # detailed explanation). This converts batch-array of Transitions
+        # to Transition of batch-arrays.
+        batch = Transition(*zip(*transitions))
+
+        # Compute a mask of non-final states and concatenate the batch elements
+        # (a final state would've been the one after which simulation ended)
+        
+        not_none_next_states = [t for t in batch.next_state if t is not None]
+        not_none_next_count = len(not_none_next_states)
+        # print(f"not_none_next_count: {not_none_next_count}")
+
+        if not_none_next_count == 0:
+            # print("Skipping optimize_model() because not_none_next_state_count == 0")
+            return
+
+        non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), device=DEVICE, dtype=torch.bool)
+
+        non_final_next_states = torch.cat(not_none_next_states)
+        state_batch = torch.cat(batch.state)
+        action_batch = torch.cat(batch.action)
+        reward_batch = torch.cat(batch.reward)
+
+        # Compute Q(s_t, a) - the model computes Q(s_t), then we select the
+        # columns of actions taken. These are the actions which would've been taken
+        # for each batch state according to policy_net
+        # action_batch_int = action_batch.type(torch.int64)
+        state_action_values = self.policy_net(state_batch).gather(1, action_batch.type(torch.int64))
+
+        # Compute V(s_{t+1}) for all next states.
+        # Expected values of actions for non_final_next_states are computed based
+        # on the "older" self.target_net; selecting their best reward with max(1).values
+        # This is merged based on the mask, such that we'll have either the expected
+        # state value or 0 in case the state was final.
+        next_state_values = torch.zeros(BATCH_SIZE, device=DEVICE)
+        with torch.no_grad():
+            next_state_values[non_final_mask] = self.target_net(non_final_next_states).max(1).values
+        # Compute the expected Q values
+        expected_state_action_values = (next_state_values * GAMMA) + reward_batch
+
+        criterion = nn.L1Loss()
+        loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))
+
+        # Optimize the model
+        self.optimizer.zero_grad()
+        loss.backward()
+        # In-place gradient clipping
+        torch.nn.utils.clip_grad_value_(self.policy_net.parameters(), 100)
+        self.optimizer.step()
+
+    def plot_scores(self, show_result=False):
+        plt.figure(1)
+        scores_t = torch.tensor(self.scores, dtype=torch.float)
+        game_lengths_t = torch.tensor(self.game_lengths, dtype=torch.float)
+        if show_result:
+            plt.title('Result')
         else:
-            next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)
+            plt.clf()
+            plt.title('Training...')
+        plt.xlabel('Episode')
+        plt.ylabel('STUFF')
+        plt.plot(scores_t.numpy(), label="scores")
+        plt.plot(game_lengths_t.numpy(), label="Game Lengths")
+
+
+        # Take 100 episode averages and plot them too
+        if len(scores_t) >= 100:
+            means = scores_t.unfold(0, 100, 1).mean(1).view(-1)
+            means = torch.cat((torch.zeros(99), means))
+            plt.plot(means.numpy(), label="MEAN scores", )
+
+            # Means of game lengths
+            mean_game_lengths = game_lengths_t.unfold(0, 100, 1).mean(1).view(-1)
+            mean_game_lengths = torch.cat((torch.zeros(99), mean_game_lengths))
+            plt.plot(mean_game_lengths.numpy(), label="MEAN Game Lengths", )
+        plt.legend()
+
+        # Plot histogram of actions
+        plt.figure(2)
+        plt.clf()
+        plt.title('Actions')
+        plt.xlabel('Action')
+        plt.ylabel('Frequency')
+
+        plt.hist(self.actions, bins=range(self.env.action_space.n))
+
+        # Plot the trend of rewards
+        plt.figure(3)
+        plt.clf()
+        plt.title('Rewards')
+        plt.xlabel('Episode')
+        plt.ylabel('Reward')
+        plt.plot(self.rewards)
+
+        # plot mean and median of rewards
+        if len(self.rewards) >= 100:
+            mean_rewards = torch.tensor(self.rewards, dtype=torch.float).unfold(0, 100, 1).mean(1).view(-1)
+            mean_rewards = torch.cat((torch.zeros(99), mean_rewards))
+            plt.plot(mean_rewards.numpy(), label="MEAN Rewards", )
+
+
+        plt.pause(0.10)  # pause a bit so that plots are updated
+
+
+    def main(self):
+        for i_episode in tqdm.tqdm(range(NUM_EPISODES)):
+            # Initialize the environment and get it's state
+            state, info = self.env.reset()
+
+            state: torch.Tensor = torch.tensor(state, dtype=torch.float32, device=DEVICE).unsqueeze(0)
+
+            for _ in count():
+                action = self.select_action(state, info["legal_moves"])
+                self.actions.append(action.item())
+
+                observation, reward, terminated, truncated, info = self.env.step(action.item())
+                reward = torch.tensor([reward], device=DEVICE) #TODO: This can be optimized by using not copy the reward to the GPU every time
+                done = terminated or truncated
 
-        # Store the transition in memory
-        memory.push(state, action, next_state, reward)
+                self.rewards.append(reward.item())
 
-        # Move to the next state
-        state = next_state
+                if terminated:
+                    next_state = None
+                else:
+                    next_state = torch.tensor(observation, dtype=torch.float32, device=DEVICE).unsqueeze(0)
 
-        # Perform one step of the optimization (on the policy network)
-        optimize_model()
+                # Store the transition in self.memory
+                self.memory.push(state, action, next_state, reward)
 
-        # Soft update of the target network's weights
-        # θ′ ← τ θ + (1 −τ )θ′
-        target_net_state_dict = target_net.state_dict()
-        policy_net_state_dict = policy_net.state_dict()
-        for key in policy_net_state_dict:
-            target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)
-        target_net.load_state_dict(target_net_state_dict)
+                # Move to the next state
+                state = next_state
 
-        if done:
-            scores.append(get_game_score(env.game_state))
-            game_lengths.append(env.game_state.turn_counter)
+                # Perform one step of the optimization (on the policy network)
+                self.optimize_model()
 
-            if show_graph_frequency is not None and i_episode % show_graph_frequency == 0:
-                plot_scores()
+                # Soft update of the target network's weights
+                # θ′ ← τ θ + (1 −τ )θ′
+                target_net_state_dict = self.target_net.state_dict()
+                policy_net_state_dict = self.policy_net.state_dict()
+                for key in policy_net_state_dict:
+                    target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)
+                self.target_net.load_state_dict(target_net_state_dict)
 
-            # if i_episode % 100 == 0:
-            #     print(f"Max score: {max(scores)}, Max game length: {max(game_lengths)}")
-            break
+                if done:
+                    self.scores.append(get_game_score(self.env.game_state))
+                    self.game_lengths.append(self.env.game_state.turn_counter)
 
-print('Complete')
+                    tensorboard_writer.add_scalar("score", get_game_score(self.env.game_state), i_episode)
+                    tensorboard_writer.add_scalar("game_length", self.env.game_state.turn_counter, i_episode)
 
-# Save the policy_net weights and results in new folder
+                    tensorboard_writer.flush()
+                    # if i_episode % SHOW_GRAPH_FREQUENCY == 0:
+                    #     self.plot_scores()
+                    break
 
+        if SHOULD_SAVE_RESULTS:
+            self.save_results()
 
-# Create a new folder for the results
-result_dir = pathlib.Path(f"results/{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}")
-result_dir.mkdir(parents=True, exist_ok=True)
+        self.plot_scores(show_result=True)
+        plt.ioff()
+        plt.show()
 
-# Save the results
-torch.save(policy_net.state_dict(), result_dir / "policy_net.pt")
-plot_scores()
-plt.savefig(result_dir / "plot.png")
 
-plot_scores(show_result=True)
-plt.ioff()
-plt.show()
+    def save_results(self):
+        # Create a new folder for the results
+        result_dir = pathlib.Path(f"results/{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}")
+        result_dir.mkdir(parents=True, exist_ok=True)
 
+        # Learning
+        # Save the results
+        torch.save(self.policy_net.state_dict(), result_dir / "policy_net.pt")
+        self.plot_scores()
+        plt.savefig(result_dir / "plot.png")
 
 
-######################################################################
-# Here is the diagram that illustrates the overall resulting data flow.
-#
-# .. figure:: /_static/img/reinforcement_learning_diagram.jpg
-#
-# Actions are chosen either randomly or based on a policy, getting the next
-# step sample from the gym environment. We record the results in the
-# replay memory and also run optimization step on every iteration.
-# Optimization picks a random batch from the replay memory to do training of the
-# new policy. The "older" target_net is also used in optimization to compute the
-# expected Q values. A soft update of its weights are performed at every step.
-#p
\ No newline at end of file
+if __name__ == "__main__":
+    trainer = Trainer()
+    trainer.main()
+    trainer.save_results()
diff --git a/evaluate.py b/evaluate.py
index 7dd1b53..eb5b7ac 100644
--- a/evaluate.py
+++ b/evaluate.py
@@ -54,8 +54,11 @@ def main():
 		RandomPlayer(),
 		RandomPlayerCopy(),
 	]
-
+	import time
+	a = time.time()
 	evaluate_players(players)
+	b = time.time()
+	print(b-a)
 
 
 if __name__ == "__main__":
diff --git a/game.py b/game.py
index 0beb302..0db7e3a 100644
--- a/game.py
+++ b/game.py
@@ -6,6 +6,7 @@ from typing import Any, Callable, DefaultDict, Dict, List, Optional, Set, Tuple
 
 import torch
 import tqdm
+import numpy as np
 
 # from pytracy import *
 # set_tracing_mode(TracingMode.All)
@@ -114,8 +115,7 @@ class ShopState:
 
 	def get_player_score(self, player_id: int) -> int:
 		# TODO: Implement other scoring rules
-		filtered_queue = [queue_item for queue_item in self.queue if queue_item.player_id == player_id]
-		score = sum(queue_item.card_type.value * queue_item.count for queue_item in filtered_queue)
+		score = sum(queue_item.card_type.value * queue_item.count for queue_item in self.queue if queue_item.player_id == player_id)
 
 		if self.limbo_item and self.limbo_item.player_id == player_id:
 			score -= self.limbo_item.item.value
@@ -301,6 +301,7 @@ class GameState:
 
 	def __str__(self):
 		return f""" {self.state}
+
 Player 0: {self.player_states[0]}
 Player 1: {self.player_states[1]}
 
@@ -542,6 +543,11 @@ def play_game_until_decision(game_state: GameState) -> None:
 			game_state.turn_counter += 1
 			game_state.state = GameStep.STATE_SHOP_0
 
+		if game_is_over(game_state):
+			game_state.state = GameStep.STATE_END
+			game_state.end_game = True
+			return
+
 	def can_player_continue(game_state: GameState, player_id: int) -> bool:
 		if sum(game_state.player_states[player_id].deck.values()) > 0:
 			return True
@@ -602,6 +608,9 @@ def play_game_until_decision_one_player_that_is_not_a_shop_decision(game_state:
 	while True:
 		play_game_until_decision_one_player(game_state, npc)
 
+		if game_is_over(game_state):
+			return
+
 		assert game_state.turn == AI_PLAYER_ID, "AI player should play"
 
 		if game_state.state == GameStep.STATE_SHOP_0_DECISION or game_state.state == GameStep.STATE_SHOP_1_DECISION:
@@ -644,10 +653,6 @@ def set_decision(game_state: GameState, decision: Optional[PlayerDecision], play
 		pass
 
 	elif game_state.state == GameStep.STATE_SHOP_0_DECISION:
-		if decision is None:
-			game_state.state = GameStep.STATE_ERROR
-			return
-
 		if decision.type != PlayerDecision.Type.SHOP_DECISION:
 			game_state.state = GameStep.STATE_ERROR
 			return
@@ -660,10 +665,6 @@ def set_decision(game_state: GameState, decision: Optional[PlayerDecision], play
 		return
 
 	elif game_state.state == GameStep.STATE_SHOP_1_DECISION:
-		if decision is None:
-			game_state.state = GameStep.STATE_ERROR
-			return
-
 		if decision.type != PlayerDecision.Type.SHOP_DECISION:
 			game_state.state = GameStep.STATE_ERROR
 			return
@@ -676,10 +677,6 @@ def set_decision(game_state: GameState, decision: Optional[PlayerDecision], play
 		return
 
 	elif game_state.state == GameStep.STATE_TURN_0 or game_state.state == GameStep.STATE_TURN_1:
-		if decision is None:
-			game_state.state = GameStep.STATE_ERROR
-			return
-
 		player_state = game_state.player_states[player_id]
 
 		if decision.type == PlayerDecision.Type.DRAW_CARD:
@@ -714,18 +711,43 @@ def set_decision(game_state: GameState, decision: Optional[PlayerDecision], play
 		# elif game_state.state == GameStep.STATE_TURN_2:
 
 			game_state.state = GameStep.STATE_END_TURN
+
+			# # Turn change logic
+			# if game_state.turn == AI_PLAYER_ID:
+			# 	game_state.turn = NPC_PLAYER_ID
+			# else:
+			# 	game_state.turn = AI_PLAYER_ID
+			# game_state.turn_counter += 1
+			# game_state.state = GameStep.STATE_SHOP_0
+
 		else:
 			assert False, "Invalid game state"
 
 		return
 
 	elif game_state.state == GameStep.STATE_END:
-		pass
+		return
 	elif game_state.state == GameStep.STATE_ERROR:
-		pass
+		return
 	else:
 		assert False, f"Invalid game state {game_state.state}"
 
+def get_legal_moves(game_state: GameState, player_id: int) -> np.ndarray:
+	actions = np.zeros(1 + len(CARD_INFO), dtype=np.int8)
+
+	if game_state.state == GameStep.STATE_TURN_0 or game_state.state == GameStep.STATE_TURN_1:
+		# Can we draw a card?
+		actions[0] = sum(game_state.player_states[player_id].deck.values()) > 0
+
+		# Can we place a card in the queue?
+		for i, (card_type, _) in enumerate(CARD_INFO):
+			actions[i+1] = game_state.player_states[player_id].hand.get(card_type, 0) > 0
+
+	# TODO: Implement GameStep.STATE_TURN_2
+	# TODO: Implement shop decision
+
+	return actions
+
 class RandomPlayer(Player):
 
 	def name(self) -> str:
diff --git a/game_env.py b/game_env.py
index 642b4fa..0aae8d8 100644
--- a/game_env.py
+++ b/game_env.py
@@ -1,10 +1,15 @@
 import gymnasium as gym
 import numpy as np
 
-from game import AI_PLAYER_ID, GameStep, PlayerDecision, get_game_score, initialize_game_state,  RandomPlayer, play_game_until_decision_one_player_that_is_not_a_shop_decision, set_decision
+from game import AI_PLAYER_ID, GameStep, PlayerDecision, game_is_over, get_game_score, get_legal_moves, initialize_game_state,  RandomPlayer, play_game_until_decision_one_player_that_is_not_a_shop_decision, print_game_state, set_decision
 
 class GameEnv(gym.Env):
 
+	metadata = {
+        "render_modes": ["human"],
+        "render_fps": 50,
+    }
+
 	def __init__(self, render_mode=None, size=5):
 		self.game_state = initialize_game_state()
 
@@ -31,10 +36,21 @@ class GameEnv(gym.Env):
 
 	def get_reward(self):
 		if self.game_state.state == GameStep.STATE_ERROR:
-			self.reward = -10000
-			return self.reward
+			assert False, "Invalid action"
+
+		reward = 0
+
+		# if self.game_state.state == GameStep.STATE_END or game_is_over(self.game_state):
+		# 	reward = get_game_score(self.game_state) * 100
+		# else:
+		reward = get_game_score(self.game_state)
+
+		# player_state = self.game_state.player_states[AI_PLAYER_ID]
+		# card_count = sum(player_state.hand.values())
 
-		return get_game_score(self.game_state) * 100 + self.game_state.turn_counter
+		# reward -= self.game_state.turn_counter
+
+		return reward
 
 	def reset(self, seed=None, options=None):
 		# We need the following line to seed self.np_random
@@ -50,7 +66,10 @@ class GameEnv(gym.Env):
 		if self.render_mode == "human":
 			self._render_frame()
 
-		return observation, dict()
+		return observation, {"legal_moves": get_legal_moves(self.game_state, AI_PLAYER_ID)}
+
+	def _render_frame(self):
+		print_game_state(self.game_state)
 
 	def step(self, action: np.ndarray):
 		play_game_until_decision_one_player_that_is_not_a_shop_decision(self.game_state, RandomPlayer())
@@ -62,9 +81,12 @@ class GameEnv(gym.Env):
 		else:
 			assert False, "Invalid action"
 
+		if self.render_mode == "human":
+			print("Decision: ", decision)
+
 		play_game_until_decision_one_player_that_is_not_a_shop_decision(self.game_state, RandomPlayer())
 
-		terminated = self.game_state.end_game or self.game_state.state == GameStep.STATE_ERROR or self.game_state.state == GameStep.STATE_END
+		terminated = self.game_state.end_game or self.game_state.state == GameStep.STATE_ERROR or self.game_state.state == GameStep.STATE_END or game_is_over(self.game_state) #TODO: This is messy
 
 		reward = self.get_reward()
 
@@ -73,5 +95,4 @@ class GameEnv(gym.Env):
 		if self.render_mode == "human":
 			self._render_frame()
 
-
-		return observation, reward, terminated, False, dict()
\ No newline at end of file
+		return observation, reward, terminated, False, {"legal_moves": get_legal_moves(self.game_state, AI_PLAYER_ID)}
\ No newline at end of file
